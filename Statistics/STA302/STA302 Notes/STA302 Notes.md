#uoft/sta302 #notes 

Notes for [STA302](../STA302.md)

---

[STA302 Sheather Textbook](attachments/STA302%20Sheather%20Textbook.pdf)
[STA302 Rencher Textbook](attachments/STA302%20Rencher%20Textbook.pdf)
[STA302 Cheat Sheet](STA302%20Cheat%20Sheet.md)

---
# Table of Contents

[#Week 1](#Week%201)
	[#Method of Least Squares](#Method%20of%20Least%20Squares)
	[#Alternative methods of Calculation](#Alternative%20methods%20of%20Calculation)
[#Week 2](#Week%202)
[#Week 3](#Week%203)
[#Week 4](#Week%204)
[#Week 5](#Week%205)
[#Week 6](#Week%206)
[#Week 7](#Week%207)
[#Week 8](#Week%208)
[#Week 9](#Week%209)
[#Week 10](#Week%2010)
[#Week 11](#Week%2011)
[#Week 12](#Week%2012)
[#Week 13](#Week%2013)

# Notes
## Week 1
> Slides: 
> 	[STA302 Module 1](attachments/STA302%20Module%201.pdf)
> 	[STA302 Katherine Module 1 Large Slides](attachments/STA302%20Katherine%20Module%201%20Large%20Slides.pdf)
> Practice: [STA302 Module 1 Practice](attachments/STA302%20Module%201%20Practice.pdf)
> R: [module1](attachments/module1.r)
> Topic: Simple Linear Regression Models and Basics

Recall
- [Random Variables](../../STA238%20Notes/Random%20Variable.md) and [Probability Distributions](../../STA238%20Notes/Probability%20Distribution.md)
- [Expected Value](../../STA237%20Notes/Expected%20Value.md)
- [Variance](../../STA237%20Notes/Variance.md)
- [Independence](../../STA237%20Notes/Independence.md)
- [Normal Distribution](../../STA237%20Notes/Normal%20Distribution.md), [T-Distribution](../../STA238%20Notes/T-Distribution.md)

Learning Goals
- Understand the basic form of simple linear regression models
- Distinguish between population regression model and fitting a regression model or a fitted regression model
- Interpret and explain the coefficients of the simple linear regression models

> "All Models are wrong, but some are useful"

A [Simple Linear Regression Model](Simple%20Linear%20Regression%20Model.md) is the formula:
$$Y=\beta_{0}+\beta_{1}x+e$$
Parameters
- Intercept parameter $\beta_{0}$
- Slope parameter $\beta_{1}$
- Error $e$ or [Residual](Residual.md)
	- Normally distributed with expected value/average $0$ and variance $\sigma^{2}$
- $Y=$ the **Response Variable** - Dependent variable
- $X=$ the **Explanatory Variable** - Independent variable, predictor variable covariate, feature
	- We use $X$ to explain/predict $Y$
- $x$ is realizations of $X$, $Y|X=x$

> Note:
> 	Note the difference between a *linear relationship* and a *statistical relationship*
> 		With a linear relationship, $y_{i}=ax_{i}+b$, is a *functional relationship*, all points determined by graph
> 		A *Statistical Relationship*, is where all $y$ follow an overall trend of a formula, but with error/deviations
> 		Difference between
> 			$Y=\beta_{0}+\beta_{1}x$ (Functional Relationship)
> 			$Y=\beta_{0}+\beta_{1}x+e$ (Statistical Relationship)
> What is a linear regression model for?
> 	A linear regression model, is a model we use to approximate the probability of a real life phenomenon
> 		Once we approximate values for our model using data of a real life phenomenon, we can extrapolate properties we know about the model, to real life
> 	Or, we can use the linear regression model to *generate* observations for a dataset, as below

A **Population Regression Model** is the underlying linear regression model for a set of data which generates observations for a dataset
- Each population sample is an independent $x_{1} Y_{1},\dots,x_{n}, Y_{n}$ where $x_{1},\dots ,x_{n}$are fixed numbers
- Our dataset may actually be generated by a population regression model, or we use this population regression model as sort of an abstraction of where the real life data comes from

> Example of a population regression model
> 	![Pasted image 20241009092021](attachments/Pasted%20image%2020241009092021.png)
> 	The randomness in the graph is due to the error, which we simulate with a normal distribution
> 	

A **Fitted Regression Model** is a model we create, given observations $y_{1}, x_{1},...,y_{n},x_{n}$ of a dataset
- Notated $\hat Y = \hat \beta_{0}+\hat\beta_{1}x$
- Where $\hat \beta_{0}$ and $\hat\beta_{1}$ *estimate* the unknown parameters of the [Population Regression Model](Population%20Regression%20Model) $\beta_{0},\beta_{1}$
	We can use the "lm" function in $R$ to estimate population parameters

> The real data is generated by a population regression model
> We take population samples which are independent $x_{1},Y_{1},...,x_{n},Y_{n}$ 
> We make a fitted regression model to estimate the population regression model

### Method of Least Squares
> When we create a fitted regression model, we want a "Line of best fit"
> Best fit in this case, means least amount of error
> To do this, we use the method of least squares

We can calculate the error, or [Residual](Residual.md) of a point in our **Fitted Regression Model**, compared to our observed dataset with: 
$$e=y_{i}-(\hat \beta_{0}+\hat\beta_{1}x_{i})$$

The [Residual Sum of Squares](Residual%20Sum%20of%20Squares.md), is the sum of all the errors/[Residuals](Residual.md) of our [Fitted Regression Model](Fitted%20Regression%20Model) squared
$$RSS=RSS(b_{0},b_{1})=\sum\limits_{i=1}^{n}[y_{i}-(b_{0}+b_{1}x_{i})]^{2}$$
	$b_{0},b_{1}$ are our estimated parameters of our [Fitted Regression Model](Fitted%20Regression%20Model)
	$x_{i},y_{i}$ is a real observation of our dataset

The [Method of Least Squares](Method%20of%20Least%20Squares.md) is a method to find $\beta_{0}$ and $\beta_{1}$ for a fitted linear regression model for a dataset 
	The method, is finding the minimizers to the [Residual Sum of Squares](Residual%20Sum%20of%20Squares.md)
We want to find $b^{*}_{0},b_{1}^{*}$ to solve:
$$\min_{b_{0},b_{1}}\sum\limits_{i=1}^{n}[y_{i}-(b_{0}+b_{1}x_{i})]^{2}$$
	This finds the best fitting line that minimizes the **RSS**

Calculating Procedure
1. Take partial derivatives of the RSS with respect to each unknown parameter
2. Set each derivative to 0 and obtain equations
3. Rearrange equations to solve for each unknown parameter

> Typical Method of Least Squares Calculation
> 	![STA302 Notes Method of Least Squares Image](STA302%20Notes%20Method%20of%20Least%20Squares%20Image.png)

For a [Simple Linear Regression Model](Simple%20Linear%20Regression%20Model.md) with two variables, the result will be
If $\sum\limits_{i=1}^{n}(x_{i}-\overline{x})^{2}>0$, then
$$b_{1}^{*}=\frac{\sum\limits_{i=1}^{n}(y-\overline{y})(x_{i}-\overline x)}{\sum\limits_{i=1}^{n}(x_{i}-\overline x)^{2}} = \frac{\sum\limits_{i=1}^{n}x_{i}y_{i}-n\overline x\overline y}{\sum\limits_{i=1}^{n}x^{2}_{i}-n\overline x^{2}}$$
$$b_{0}^{*}=\overline y-b_{1}^{*}\overline x$$
	Or $b_{1}^{*}=  \frac{\text{Sample Covariance}}{\text{Sample Variance}}$ 

> Formulas for [Covariance](../../STA237%20Notes/Covariance.md) and [Variance](../../STA237%20Notes/Variance.md) may help
> 	$Cov(X,Y)=E[XY]-E[X]E[Y]$
> 	$Var(X)=E[X^2]-(E[X])^2$

## Week 2
> Slides: 
> 	[STA302 Module 2](attachments/STA302%20Module%202.pdf)
> 	[STA302 Katherine Module 2 Large Slides](attachments/STA302%20Katherine%20Module%202%20Large%20Slides.pdf)
> Practice: [STA302 Module 2 Practice](attachments/STA302%20Module%202%20Practice.pdf)
> R:
> 	[module2](attachments/module2.r)
> 	[module2_plot](attachments/module2_plot.r)

### Simple Linear Regression
The [Simple Linear Regression Model](Simple%20Linear%20Regression%20Model.md) can be written in matrix form as:
$$ Y=\begin{bmatrix} Y_{1}\\ \vdots \\ Y_{n}\end{bmatrix}=\begin{bmatrix} (1,x_{1})\beta+e_{1} \\ \vdots \\ (1,x_{n})\beta + e_{n}\end{bmatrix}$$
We can define 
$$X=\begin{bmatrix}  1 & x_{1}\\ \vdots &  \vdots \\ 1  &  x_{n}\end{bmatrix}, \ \ \ \beta=\begin{bmatrix} b_{0} \\ b_{1}\end{bmatrix} \ \ \ e=\begin{bmatrix} e_{1} \\ \vdots  \\ e_{n}\end{bmatrix}$$
where we can write the simple linear regression model as:
$$Y=X\beta + e$$

>Matrix form exists, so that given a vector of $x$, we can plug it into the [Simple Linear Regression Model](Simple%20Linear%20Regression%20Model.md) and get a vector $Y$ of the results
>	It describes calculating the entire set of data at once

Our eqations from 

### Multiple Linear Regression
> The [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) is the [Linear Regression Model](Linear%20Regression%20Model) with multiple **Explanatory Variables** which describe one **Response Variable**

The formula for a [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) is generally:
$$ Y_{i}=b_{0}+b_{1}x_{i}+...+b_{p}x_{p}+e$$
Where
- $Y$ is a random univariate dependent variable
- The **Vector of Predictors** is $x=(1,x_{1},...,x_{p})^{T}$
	- A $p+1$ dimension vector of intercept, and $p$ explanatory/predictor variables
- $e\sim N(0,\sigma^{2})$
	- $\sigma^{2}$ is an error variance parameter
- $E(Y|X=x)=E(Y|X_{1}=x_{1},...,X_{p}=x_{p})=b_{0}+b_{1}x_{1}+...+x_{p}b_{p}$
- $Var(Y|X=x)=Var(Y|X_{}1=x_{1},...,X_{p}=x_{p})=\sigma^{2}$
 
 The [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) can also be written as:
$$Y_{i}=x_{i}^{T}\beta +e_{i}$$
for $i=1,...,n$ where
- $\beta$ is a parameter vector of dimension $p+1$ 
- $e$ is the vector of the errors of dimension $1$
- $x_{i}$ is a vector of the predictors $x_{i}=(1,x_{1},...,x_{p})^{T}$

> For a multiple linear regression model, each $y$ has a *set of $x$* to describe it
> And so $\beta$ is also a set 
> I think whenever they do $x_{i}^{T}$, or anything like that, they are saying that so that the vector is in the correct orientation, to make [Matrix Multiplication](../../../Mathematics/MAT223%20Notes/Matrix%20Multiplication.md) possible


The [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) can be written in **Matrix Form** as:
$$Y=X\beta+e$$
where
$$X=\begin{bmatrix} x^{T}_{1}\\\vdots\\x^{T}_{n}\end{bmatrix}=\begin{bmatrix} 1 & x_{1,1} & ... & x_{1,p} \\ \vdots\\ 1 & x_{n,1} & ... & x_{n,p}\end{bmatrix}$$
$$\beta = \begin{bmatrix} b_{0}\\ \vdots \\ b_{p}\end{bmatrix} \ \ \ e=\begin{bmatrix} e_{1}\\\vdots \\ e_{n}\end{bmatrix}\sim N(0_{n},\sigma^{2} I_{n})$$
- $Y$ is a random univariate dependent variable
- The **Vector of Predictors** is $X=(1,x_{1},...,x_{p})^{T}$
	- Where each row/$x_{i}$ is a vector of the predictors $x_{i}=(1,x_{1},...,x_{p})^{T}$ for a point of data
	- Each column, represents the information of one predictor, for all the points
		- First column is always just 1's to represent the constant intercept
	- There are $p$ explanatory/predictor variables
	- The vector of predictors is $p+1$ dimensions (+1 for intercept)
- $e\sim N(0,\sigma^{2})$
	- $\sigma^{2}$ is an error variance parameter
- $E(Y|X=x)=E(Y|X_{1}=x_{1},...,X_{p}=x_{p})=b_{0}+b_{1}x_{1}+...+x_{p}b_{p}$
- $Var(Y|X=x)=Var(Y|X_{}1=x_{1},...,X_{p}=x_{p})=\sigma^{2}$
- Can also be written as $Y_{i}=x_{i}^{T}\beta +e_{i}$
- Written in matrix form as $Y=X\beta+e$


>Again, matrix form exists, so that given a vector of $x$, we can plug it into the [Simple Linear Regression Model](Simple%20Linear%20Regression%20Model.md) and get a vector $Y$ of the results

### Qualitative/Categorical Predictors
In general, for numerical predictors, each column in the matrix of predictors represents the value of that predictor for all points

For categorical predictors, we use *indicator variables* to indicate the value of each entry.
	For each value that the categorical predictor can have, there is a seperate variable to indicate the value of that category

Suppose variable $x_{3}$ is qualitative with values (Yes, No, Maybe)
Then we may have indicator variables such as:
$$x_{3}=\begin{cases} 1, \text{if Yes} \\0, \text{otherwise}\end{cases}, \ \ \ x_{4}=\begin{cases} 1, \text{if No} \\0, \text{otherwise}\end{cases}$$
And our matrix of predictors may look like
$$X=\begin{bmatrix} 1 & 2 & 1 & 0 \\ 1 & 3 & 0 & 1 \\ \vdots & \vdots & \vdots & \vdots \\  1 & 3 & 0 & 0 \\ 1 & 3 & 0 & 1\end{bmatrix}$$
	Where the last two columns, represent the categorical predictor
	For the first point, the categorical variable is set to Yes
	For the second point, the categorical variable is set to No

### Least Squares on Multiple Linear Models
Choosing a vector $b^{*}$ of dimension $p+1$, the [Residual](Residual.md) of a [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) given $y,x$ is
$$e=y_{i}-x_{i}^{T}b^{*}$$

The vector of all residuals is denoted:
$$E=Y-Xb^{*}$$

An the [Residual Sum of Squares](Residual%20Sum%20of%20Squares.md) is:
$$\sum\limits_{i=1}^{n}(y_{i}-x_{i}^{T}b)^{2}=\sum\limits_{i=1}^{n}(y_{i}-(b_{0}x_{i,1}+\dots+b_{p}x_{i,p}))^{2} =(y-Xb)^{T}(y-Xb)$$

The [Method of Least Squares](Method%20of%20Least%20Squares.md) works similarly here, but now uses matrix operations.
Our process is still to:
1. Take partial derivatives with respect to each parameter
2. Set derivatives to 0 and obtain equations
3. Rearrange equations to solve for each unknown parameter

> Solving Multiple Linear Regression Method of Least Squares
> ![STA302 Notes Multiple Linear Regression Method](STA302%20Notes%20Multiple%20Linear%20Regression%20Method.png)

Solving these large matrices can be tricky because they are so large
	Some tips for calculation
		$X^{T}X=\begin{bmatrix} n & \sum\limits x_{1}  & \sum\limits x_{2} & \dots  & \sum\limits x_{p} \\ \sum\limits x_{1}  & \sum\limits x_{1}^{2} & \sum\limits x_{1} x_{2}  & \dots  & \sum\limits x_{1}x_{p} \\ \sum\limits x_{2} & \sum\limits x_{1} x_{2}  & \sum\limits x^{2}  & \dots  &  \sum\limits x_{2} x_{p}  \\ \vdots & \vdots  & \vdots &  \dots & \vdots \\ \sum\limits x_{p} & \sum\limits x_{1}x_{p} & \sum\limits x_{2}x_{p} & \dots  & \sum\limits x_{p}^{2} \end{bmatrix}$
			$n$ is number of columns in $X$
			each $x_{i}$ is sum of the values of that predictor
		$X^{T}Y=\begin{bmatrix} \sum\limits y \\ \sum\limits x_{1}y  \\ \vdots \\ \sum\limits x_{p}y\end{bmatrix}$
			each $x_{i}$ is sum of the values of that predictor
		Each value with $\sum\limits x_{1}$ or $\sum\limits y$ means $\sum\limits x_{i,1}$ or $\sum\limits y_{i}$, representing sum of the entire column of that predictor

We can calculate the least squares estimator as
$$\hat\beta=(X^{T}X)^{-1}X^{T}Y$$
Note
- $X^{T}X$ usually needs aid of software, so is often provided in a question

### Interactions
We can create *interaction variables* to allow the relationship between response and one predictor, to vary according to values of a second predictor

## Week 3
> Slides: 
> 	[STA302 Module 3](attachments/STA302%20Module%203.pdf)
> 	[STA302 Katherine Module 3 Large Slides](attachments/STA302%20Katherine%20Module%203%20Large%20Slides.pdf)
> Practice: [STA302 Module 3 Practice](attachments/STA302%20Module%203%20Practice.pdf)
> R: [module3](attachments/module3.rmd)

Goals for this week
- Know regression modeling assumptions
	- Be able to plot and interpret plots in relation to assumptions 

For this week
- Always assume [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model)
- $\hat \beta, \hat\sigma^{2}$ are both [Estimators](../../STA238%20Notes/Estimator.md)
	- $b^*$ is the computed/realized value of $\hat\beta$ 
- $\hat \beta, \hat\sigma^{2}$ are both random, based on regression model assumptions
- Given a dataset, compute $\hat \beta, \hat\sigma^{2}$ means compute using given data

### Sampling Distributions
[Sampling Distribution](../../STA238%20Notes/Sampling%20Distribution.md) for a [Residual](Residual.md) of [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) satisfies:
- $E(\hat e_{i}|X)=0$
- $Var(\hat e_{i}|X)=\sigma^{2}(1-h_{i,i})$
	- $h_{i,i}$ are entries of [Hat Matrix](Hat%20Matrix): $H=X(X^{T}X)^{-1}X^{T}$
- If errors $e_{i}$ are normally distributed, residuals are normally distributed

The [Residual](Residual.md) of a [Multiple Linear Regression Model](Multiple%20Linear%20Regression%20Model) is random
- The [Sampling Distribution](../../STA238%20Notes/Sampling%20Distribution.md) for a residual $\hat e_{i}$ satisfies
	- $E(\hat e_{i}|X)=0$
	- $Var(\hat e_{i}|X)=\sigma^{2}(1-h_{i,i})$
		- $h_{i,i}$ are entries of [Hat Matrix](Hat%20Matrix): $H=X(X^{T}X)^{-1}X^{T}$
	- If errors $e_{i}$ are normally distributed, residuals are normally distributed

The [Least Squares](Method%20of%20Least%20Squares.md) solution $\hat b_{0}, \hat b_{1}$ calculated for  a given $Y_{1},...,Y_{n}$ is **random**
- The distribution of $\hat b_{0}, \hat b_{1}$ follows a [Sampling Distribution](../../STA238%20Notes/Sampling%20Distribution.md)

The estimated variance of $\hat \sigma^{2}$ Is random with a sampling distribution

### Assumptions for Regression Model
Whenever we fit a model, we implicitly make these assumptions **EVERY** time
Regression Model Assumptions
- Linearity Assumption 
	- Assuming that:
		- $E(Y|X=x)=b_{0}+b_{1}x_{1}+...+b_{p}x_{p} = X\beta$ or
		- $E(e|X)=0$ or 
		- $Y=X\beta+\epsilon$ 
	- Also known as "Mean Zero Errors" assumption
	- We assume that the expected value of our model, is the same as the real value
	- This means we are assuming:
		- that the true relationship is linear in the coefficients
		- And also assume that the true relationship is exactly $Y=X\beta +\epsilon$ with no incorrect predictors
- Constant error variance (homoscedasticity)
	- Assuming that:
		- $Var(Y|X=x)=\sigma^{2}$
	- Assume variance of error given by predictors, does not change with value of predictors
	- This means we are assuming:
		- Each conditional distribution has the same spread
		- Ensures reasonable estimates of variablity for all conditional means
- Independent and normal errors
	- Independent Errors
		- $Cov(\epsilon_{i},\epsilon_{j})=0$
		- $Cov(y_{i},y_{j})=0$
	- Normal Errors
		- $\epsilon_{i}\sim N(0,\sigma^{2})$ 
	- Residuals/errors are [Independent Identically Distributed Random Variables](../../STA238%20Notes/Independent%20Identically%20Distributed%20Random%20Variables.md) of $N(0,\sigma^{2})$
	- This means we are assuming that
		- Each data point in population is not related or connected to any other data point
		- Each conditional distribution has the same shape
	- This
		- Ensures correct precision of estimates
		- Allows us to utilize properties of normal random variables
		
- There is no perfect way for checking regression modeling assumptions

> Our goal in this section, is to be able to check if our assumptions hold
> Note that there is no perfect way to check our assumptions, our methods are not always correct

For Multiple Linear Regression, we have some additional conditions
- Conditional Mean Response Condition
	- Make sure the mean responses are a single function of a linear combination involving $\beta$
	- $E(Y_{i}|X=x_{i})=g(\beta_{0}+\beta_{1}x_{i,1}+\dots+\beta_{p}x_{i,p})$
-  Conditional Mean Predictor Condition
	- Make sure mean of each predictor, is related to other predictors in no more complicated way than linearly
	- $E(X_{i}|X_{j})=a_{0}+a_{1}X_{j}$
	- Linear or no relationship satisfy condition, anything else violates

Recall that the [Sampling Distribution](../../STA238%20Notes/Sampling%20Distribution.md) for a [Residual](Residual.md) $\hat e_{i}$ satisfies
- $E(\hat e_{i}|X)=0$
- $Var(\hat e_{i}|X)=\sigma^{2}(1-h_{i,i})$
	- $h_{i,i}$ are entries of [Hat Matrix](Hat%20Matrix): $H=X(X^{T}X)^{-1}X^{T}$
- If errors $e_{i}$ are normally distributed, residuals are normally distributed
- Can be calculated with `resid(fit)`

A [Standardized Residual](Standardized%20Residual) is a [Residual](Residual.md) which is transformed so that they are approximately $N(0,1)$
$$r_{i}= \frac{\hat e_{i}}{\hat \sigma \sqrt{1-h_{i,i}}}$$
- Standardized Residuals are correlated
- They should be *approximately* $N(0,1)$, but not exactly
- Can be calculated with `rstandard(fit)`

What we should look for in our model
- Residual vs Fitted Values or Predictors Plot should:
	- be uncorrelated with fitted values and predictors
	- Have constant mean 0
	- residual plot should look like null plot(no pattern)
- QQ Plots should
	- Follow Diagonal line, maybe curving off at edges
	- No significant patters throughout
- For Multiple Linear Response
	- Results/Response vs Fitted Values Plot should have diagonal scatter, or easily identifiable non-linear trend
	- Scatterplots of relationships between predictors, *Pairwise Scatterplots* should have no curves, non-linear patterns 
- Additionally
	- Variance of residuals not constant but close to 1
	- residuals are correlated, but often not visible
	- standardized residuals should constant mean 0, correlated, and roughly have standard normal

> Example of Good Residual Plots
> 	![425](attachments/STA302%20Notes%20Good%20Residual%20Plot%20Image.png)
> Example of Bad Residual Plots
> 	![246](attachments/STA302%20Notes%20Bad%20Residual%20Plot%20Image.png)![254](attachments/STA302%20Notes%20Bad%20Residual%20Plots.png)
> Example of Good and Bad QQ Plots
> 	![234](attachments/STA302%20Notes%20Good%20and%20Bad%20QQ%20Plots%20Image.png)
> Example of good Response versus Fitted Values, Pairwise Scatterplots
> 	![STA302 Notes Response vs Fitted Values](attachments/STA302%20Notes%20Response%20vs%20Fitted%20Values.png)

## Week 4
> Slides: 
> 	[STA302 Module 4](attachments/STA302%20Module%204.pdf)
> 	[STA302 Katherine Module 4 Large Slides](attachments/STA302%20Katherine%20Module%204%20Large%20Slides.pdf)
> Practice: [STA302 Module 4 Practice](attachments/STA302%20Module%204%20Practice.pdf)
> R: 
> 	[module4](attachments/module4.r)
> 	[module4_activity_template](attachments/module4_activity_template.rmd)

Goals
- Extend models we use to allow *interactions* between predictors
	- Predictors with different effect on response for different values of another predictor
- Implement polynomial regression

### Interactions
We can model effects of predictors having different effects on response for different values of another predictor with multiplications:
$$Y=b_{0}+b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{1}x_{2}$$
- Here 2 "main effects" and 1 "interaction term"
- Can also write $x_{3}=x_{1}x_{2}$ 
- R Code for modeling interactions
``` R 
# Use colon for interactions . Can also use star .
fitted_ model = lm(Petal.Length ~ Sepal.Width + Sepal.Length + Sepal.Width:Sepal.Length , data = iris)
```

Polynomial Regression
- Can add new predictors as powers of existing predictor variable
$$Y=b_{0}+b_{1}x+b_{2}x^{2}+\dots+b_{k}x^{k}+e$$
	- Still linear when we define $x_{1}=x, x_{2}=x^{2}$?

What to do with plots look "problematic"
- If diagnostic plots of assumptions provide strong evidence against regression model, then we need to adjust our model 
- We can
	- Transform predictors
	- Transform response
	- Transform both
- Overall we want to pick predictors to fix residuals plots


[Variance Stabilizing Transformations](Variance%20Stabilizing%20Transformations) specifically target the violation of constant variance
- We can solve that $Var(f(Y))=[f'(E(Y))]^{2}Var(Y)$, with this information, we know how to affect the variance of $Y$, by applying a transformation to the response
- If facing violation of constant variance, attempt to apply transformation on $Y$/Model Response
- These transformations can be applying any kind of function to $Y$, Log, Exponential, etc...

If a violation of *normality* is observed, we can use the [Box-Cox Method](Box-Cox%20Method.md)
The [Box-Cox Method](Box-Cox%20Method.md) is a method that picks the best **Power Transformation** for a predictor or the response which approximates normality
- can be used on the response, predictors, or both simultaneously
- Mostly done via computer
- If **Box-Cox** estimates a value of $\lambda=0.3$, than the response $Y$ should be raised to the power $\lambda=0.3$ to best approximate normality

> Remember:
> 	 Violating normality means that the residuals do not approximate follow normal distribution

``` R
# Box-Cox R Example
# Choose lambda 
boxcox (fit) 
lambda = 1 / 2 

# Transform and fit 
gm = prod ( y ) ^(1 / n ) 
y_transformed = gm^(1 - lambda ) * ( y^lambda - 1) / lambda
transformed_fit = lm( y_transformed ~ x )
```

Depending on our issues with the linear regression model, we can try different
- Issues with residual variance/distribution
	- Transform response to stabilize variance and/or fix distribution of residuals
- Issues with linearity
	- Transform predictors

Other Transformations and what they should be used for:
- Help improve residual right skews
	- Natural Log $g(x)=ln(x)$
	- Square Root
- Help improve residual left skews
	- Power: $g(x)=x^{\lambda}$ for $\lambda>0$
	- Cube Root
	- Other Logarithms

Interpreting Transformations
- Remember to incorporate variables as they are after transformatinos
	- If a predictor was squared, then we say "for a one-unit increase in $x^{2}$", not in $x$
## Week 5
> Slides: 
> 	[STA302 Module 5 Slides](attachments/STA302%20Module%205%20Slides.pdf)
> 	[STA302 Katherine Module 5 Large Slides](attachments/STA302%20Katherine%20Module%205%20Large%20Slides.pdf)
> Practice: [STA302 Module 5 Practice](attachments/STA302%20Module%205%20Practice.pdf)
> R: [module5_activity_template](attachments/module5_activity_template.rmd)

Covariance Matrices combine information about
- Variance of each individual random variable
- How any two random variables vary together
You can read the Covariance matrix of a sampling distribution as:
$$Cov(\hat\beta|X)=\begin{bmatrix} Var(\hat\beta_{0}|X)  &  Cov(\hat\beta_{0}, \hat\beta_{1}|X)  & \dots  &  Cov(\hat\beta_{0}, \hat\beta_{p}|X)  \\  Cov(\hat\beta_{0}, \hat\beta_{1}|X)  &  Var(\hat\beta_{1}|X)  & \dots  &  Cov(\hat\beta_{1}, \hat\beta_{p}|X)  \\ \vdots  &  \vdots  & \ddots & \vdots \\ Cov(\hat\beta_{0}, \hat\beta_{p}|X)  &  Cov(\hat\beta_{1},\hat\beta_{p}|X) & \dots  &  Var(\hat\beta_{p}|X) \end{bmatrix} 
$$
And you can calculate it as:
$$Cov(\hat\beta|X)=\sigma^{2}(X^{T}X)^{-1}$$
For single linear regression,
$$Cov(\hat\beta|X)=\begin{bmatrix} \end{bmatrix}$$ fill out from slides page 5
You can use this to easily different values of variance and covariance, just calculate the matrix, and use reference the value

We create a *Sampling Distribution* for our predictor, in order to determine what is normal variance to expect, and to determine if our estimated value is reasonable or not

The distribution of the our estimate of the predictors given a dataset, should be:
$$\hat\beta \sim N(\beta,\sigma^{2}(X^{T}X)^{-1})$$
- $\hat\beta$ represents the random variable, of different value for coefficients we get given different samples of the dataset
- $\beta$ is the fitted coefficients we calculate

We usually only do this on one dataset however, so we don't have a value for $\sigma^{2}$, and we need to use an estimate of $\sigma^{2}$
We can estimate $\sigma^{2}$ with $\sigma^{2}= \frac{RSS}{n-p-1}$

Using estimated variance now introduces more variation though, so we compare our distribution with the [T-Distribution](../../STA238%20Notes/T-Distribution.md), where:
$$\frac{\hat\beta-\beta}{\sqrt{\sigma^{2}(X^{T}X)^{-1}}}\sim T_{n-p-1}$$

With this sampling distribution, we can now construct a $(1-a)$% [Confidence Interval](../../STA238%20Notes/Confidence%20Interval.md) for each predictor with:
$$\hat\beta_{j}\pm t_{\frac{a}{2},n-p-1}\sigma\sqrt{(X^{T}X)^{-1}_{(j+1,j+1)})}$$
A Hypothesis Test uses these confidence intervals to determine the probability that $\hat\beta_{j}=0$, and that no linear relationship exists with that predictor




## Week 6
> Midterm Exam


## Week 7
> Slides:
> 	[STA302 Module 6 Slides](attachments/STA302%20Module%206%20Slides.pdf)
> 	[STA302 Katherine Module 6 Large Slides](attachments/STA302%20Katherine%20Module%206%20Large%20Slides.pdf)
> Practice: [STA302 Module 6 Practice](attachments/STA302%20Module%206%20Practice.pdf)
> R: [module6](attachments/module6.r)


## Week 8
> Slides: 
> 	[STA302 Module 7 Slides](attachments/STA302%20Module%207%20Slides.pdf)
> 	[STA302 Katherine Module 7 Large Slides](attachments/STA302%20Katherine%20Module%207%20Large%20Slides.pdf)
> Practice: [STA302 Module 7 Practice](attachments/STA302%20Module%207%20Practice.pdf)
> R:
> 	[module7](attachments/module7.r)
> 	[module7_activity](attachments/module7_activity.rmd)


## Week 9
> Reading Week


## Week 10
> Slides:
> 	[STA302 Module 8 Slides](attachments/STA302%20Module%208%20Slides.pdf)
> 	[STA302 Katherine Module 8 Large Slides](attachments/STA302%20Katherine%20Module%208%20Large%20Slides.pdf)
> Practice: [STA302 Module 8 Practice](attachments/STA302%20Module%208%20Practice.pdf)
> R:
> 	[module8](attachments/module8.r)
> 	[module8_activity](attachments/module8_activity.rmd)

## Week 11
> Slides:
> 	[STA302 Module 9 Slides](attachments/STA302%20Module%209%20Slides.pdf)
> 	[STA302 Katherine Module 9 Large Slides](attachments/STA302%20Katherine%20Module%209%20Large%20Slides.pdf)
> R: [module9](attachments/module9.r)

## Week 12
> Slides:
> 	[STA302 Module 10 Slides](attachments/STA302%20Module%2010%20Slides.pdf)
> 	[STA302 Katherine Module 10 Large Slides](attachments/STA302%20Katherine%20Module%2010%20Large%20Slides.pdf)
> Practice: [STA302 Module 10 Practice](attachments/STA302%20Module%2010%20Practice.pdf)
> R: [module10](attachments/module10.r)

## Week 13
> Final Exam Review: 
> 	[STA302 Week 13 Exam Review](STA302%20Week%2013%20Exam%20Review.pdf)
> 	[STA302 Katherine Final Exam Review](attachments/STA302%20Katherine%20Final%20Exam%20Review.pdf)
